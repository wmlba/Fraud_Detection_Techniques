{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Credit Card Fraud Prediction with XGBoost\n",
    "_**Using Gradient Boosted Trees to Predict Fraudulent Transactions**_\n",
    "\n",
    "### All the code is on this github repo: https://github.com/wmlba/Fraud_Detection_Techniques\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "\n",
    "\n",
    "Identifying Credit Card Fraudulent transaction is crucial to the banking business to protect their customers from fraud. Identifying fraud transactions is not an easy task and it can consume a lot of auditors time to identify those transactions and it can delay the purchasing of whatever items the card holder wants to purchase.\n",
    "\n",
    "In this notebook, I will demonstrate how to build an binary classification model to predict whether a specific credit card transaction is genuine of fraud. I will use Sagemaker's implementation of XGBoost algorithm.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Machine Learning Process\n",
    "\n",
    "<img src=\"./images/MLProcess.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Decision Trees and XGBoost\n",
    "\n",
    "<img src=\"./images/decision_trees2.png\" width=\"700\" height=\"700\">\n",
    "<img src=\"./images/decision_trees.png\" width=\"700\" height=\"700\">\n",
    "\n",
    "\n",
    "**Source:** https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "\n",
    "\n",
    "### Why XGBoost? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "#Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "#manipulating entities and resources that Amazon SageMaker uses, such as training jobs, endpoints, and input datasets in S3.\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'DEMO-xgboost-fraud-detection'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Python libraries we'll need for the remainder of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pre-requisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "The dataset we use is publicly available at https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universit√© Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n",
    "\n",
    "Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o ./data/creditcardfraud.zip -d ./data\n",
    "credit_df = pd.read_csv('./data/creditcard.csv')\n",
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "<img src=\"./images/MLProcess2.png\" width=\"200\" height=\"200\">\n",
    "\n",
    "### 1- Get The Dataframe Summary\n",
    "\n",
    "Looks like the ```Time``` and ```Amount``` features need scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the credit_df dataframe after scaling\n",
    "credit_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Checking The Balance of The Data\n",
    "\n",
    "Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of Non-Frauds are: ' + str(credit_df['Class'].value_counts()[0]) + ' which is ', round(credit_df['Class'].value_counts()[0]/len(credit_df) * 100,2), '% of the dataset')\n",
    "print('The number of Frauds are: ' + str(credit_df['Class'].value_counts()[1]) + ' which is ', round(credit_df['Class'].value_counts()[1]/len(credit_df) * 100,2), '% of the dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('Class', data=credit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Checking Missing Data\n",
    "\n",
    "If there are any missing data in our dataset, we need to deal with them before training. Data Imputation is a critical step in the Feature Engineering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Feature Correlation\n",
    "\n",
    "A correlation matrix is a way to show the correlation between different variables in a dataset. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data and find what , as an input into a more advanced analysis\n",
    "\n",
    "**Positive Correlation means:** As feature X increases, feature Y increases and vice versa.\n",
    "\n",
    "**Negative Correlation means:** As feature X go in one direction (Increase), feature Y goes in the other direction (Decrease)\n",
    "\n",
    "Let's plot the first correlation matrix for the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample figsize in inches\n",
    "fig, ax = plt.subplots(figsize=(20,10))         \n",
    "\n",
    "# Imbalanced DataFrame Correlation\n",
    "corr = credit_df.corr()\n",
    "sns.heatmap(corr, cmap='YlGnBu', annot_kws={'size':30}, ax=ax)\n",
    "ax.set_title(\"Imbalanced Correlation Matrix\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "1- Scale the features that needs scaling.\n",
    "\n",
    "2- Fix the target feature imbalance problem.\n",
    "\n",
    "3- Detect and Remove Outliers.\n",
    "\n",
    "3- Split the data to train, validation and test.\n",
    "\n",
    "<img src=\"./images/fengineering.png\" width=\"200\" height=\"200\">\n",
    "\n",
    "### 1- Fixing The Feature Scaling Problem For (Time and Amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# RobustScaler is robust to outliers.\n",
    "\n",
    "credit_df['amount_after_scaling'] = RobustScaler().fit_transform(credit_df['Amount'].values.reshape(-1,1))\n",
    "credit_df['time_after_scaling'] = RobustScaler().fit_transform(credit_df['Time'].values.reshape(-1,1))\n",
    "\n",
    "credit_df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "# Place the class in the begining of the dataframe\n",
    "Class = credit_df['Class']\n",
    "credit_df.drop(['Class'], axis=1, inplace=True)\n",
    "credit_df.insert(0, 'Class', Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see immediately that:\n",
    "- `time_after_scaling` appears to be quite evenly distributed and scaled with min and max values are within the range of the other features\n",
    "- `amount_after_scaling` transaction amount values are also scaled to be within the range of the other features with min of -0.307 and max of 358.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Fix the target feature imbalance problem (Resampling)\n",
    "\n",
    "<img src=\"./images/resampling.png\">\n",
    "\n",
    "#### Undersampling The Class Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the Dataset.\n",
    "shuffled_df = credit_df.sample(frac=1,random_state=4)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_df = shuffled_df.loc[shuffled_df['Class'] == 1]\n",
    "\n",
    "#Randomly select 492 observations.\n",
    "non_fraud_df = shuffled_df.loc[shuffled_df['Class'] == 0].sample(n=492,random_state=42)\n",
    "normalized_df = pd.concat([fraud_df, non_fraud_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('Class', data=normalized_df)\n",
    "plt.title('Balanced Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling The Class Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(ratio='minority', random_state=7)\n",
    "\n",
    "oversampled_trainX, oversampled_trainY = sm.fit_sample(credit_df.drop('Class', axis=1), credit_df['Class'])\n",
    "oversampled_train = pd.concat([pd.DataFrame(oversampled_trainY), pd.DataFrame(oversampled_trainX)], axis=1)\n",
    "oversampled_train.columns = normalized_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('Class', data=oversampled_train)\n",
    "plt.title('Balanced Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check The Correlation Matrix After Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))         \n",
    "\n",
    "# Imbalanced DataFrame Correlation\n",
    "corr = normalized_df.corr()\n",
    "sns.heatmap(corr, cmap='YlGnBu', annot_kws={'size':30}, ax=ax)\n",
    "ax.set_title(\"Imbalanced Correlation Matrix\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative Correlations:** Some features, such as V17, V14, V12 are negatively correlated with the target class. That means that the lower these values, the more likely the transaction will be a fraudulent.\n",
    "\n",
    "**Positive Correlations:** Features V2, V4, V11 are positively correlated. That means that the higher these values, the more likely the transaction will be a fraudulent.\n",
    "\n",
    "Now, let's find any outliers in those features:\n",
    "\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot (Detecting Outliers)\n",
    "\n",
    "<img src=\"./images/plotbox.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(20,4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V17\", data=normalized_df, ax=axes[0])\n",
    "axes[0].set_title('V17')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=normalized_df, ax=axes[1])\n",
    "axes[1].set_title('V14')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=normalized_df, ax=axes[2])\n",
    "axes[2].set_title('V12')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Remove Outliers and re-plot the Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V17_non_fraud = normalized_df.query('V17 > -7 & Class == 0')\n",
    "V17_fraud = normalized_df.query('Class == 1')\n",
    "normalized_df_no_outlier = pd.concat([V17_non_fraud,V17_fraud])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Check the Boxplot after removing the major outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(20,4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V17\", data=normalized_df_no_outlier, ax=axes[0])\n",
    "axes[0].set_title('V17')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=normalized_df_no_outlier, ax=axes[1])\n",
    "axes[1].set_title('V14')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=normalized_df_no_outlier, ax=axes[2])\n",
    "axes[2].set_title('V12')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the normalized dataset before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_normalized_df = normalized_df.sample(frac=1)\n",
    "shuffled_normalized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Split the train/test data (Used For Evaluation Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(shuffled_normalized_df, [int(0.7 * len(shuffled_normalized_df)), int(0.9 * len(shuffled_normalized_df))])\n",
    "train_data.to_csv('./data/train.csv', header=False, index=False)\n",
    "validation_data.to_csv('./data/validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload these files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('./data/train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('./data/validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "At this point you have successfully imported your data into Amazon Forecast and now it is time to get started in the next notebook to build your first model. To Continue, execute the cell below to store important variables where they can be used in the next notebook, then open **02_Training and Tuning.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_data \n",
    "%store validation_data \n",
    "%store test_data\n",
    "%store shuffled_normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
