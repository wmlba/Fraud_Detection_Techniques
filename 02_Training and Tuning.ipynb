{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore Variables from Previous Notebook\n",
    "\n",
    "This part will restore the variables that was stored in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Classifier on Amazon SageMaker\n",
    "\n",
    "We will train a decision tree based classifier using SageMaker's built-in Algorithm (XGBoost)\n",
    "\n",
    "## Amazon SageMaker\n",
    "Amazon SageMaker Amazon SageMaker is a fully managed machine learning service that automates the end-to-end ML process. With Amazon SageMaker, data scientists and developers can quickly and easily build and train machine learning models and directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. With native support for bring-your-own-algorithms and frameworks, Amazon SageMaker offers flexible distributed training options that adjust to your specific workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s).\n",
    "- Specify the locations of the XGBoost algorithm containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "#Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "#manipulating entities and resources that Amazon SageMaker uses, such as training jobs, endpoints, and input datasets in S3.\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'DEMO-xgboost-fraud-detection'\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "Binary classification error rate. It is calculated as #(wrong cases)/#(all cases)\n",
    "\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-20 09:29:54 Starting - Starting the training job...\n",
      "2019-10-20 09:29:56 Starting - Launching requested ML instances......\n",
      "2019-10-20 09:30:59 Starting - Preparing the instances for training......\n",
      "2019-10-20 09:32:07 Downloading - Downloading input data...\n",
      "2019-10-20 09:32:40 Training - Downloading the training image..\u001b[31mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[31mReturning the value itself\u001b[0m\n",
      "\u001b[31mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[09:33:02] 688x30 matrix with 20640 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[09:33:02] 197x30 matrix with 5910 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[31mINFO:root:Train matrix has 688 rows\u001b[0m\n",
      "\u001b[31mINFO:root:Validation matrix has 197 rows\u001b[0m\n",
      "\u001b[31m[0]#011train-error:0.031977#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[1]#011train-error:0.023256#011validation-error:0.101523\u001b[0m\n",
      "\u001b[31m[2]#011train-error:0.015988#011validation-error:0.096447\u001b[0m\n",
      "\u001b[31m[3]#011train-error:0.010174#011validation-error:0.106599\u001b[0m\n",
      "\u001b[31m[4]#011train-error:0.007267#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[5]#011train-error:0.005814#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[6]#011train-error:0.001453#011validation-error:0.096447\u001b[0m\n",
      "\u001b[31m[7]#011train-error:0.001453#011validation-error:0.096447\u001b[0m\n",
      "\u001b[31m[8]#011train-error:0.001453#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[9]#011train-error:0.001453#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[10]#011train-error:0.001453#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[11]#011train-error:0.001453#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[12]#011train-error:0.001453#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[13]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[14]#011train-error:0#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[15]#011train-error:0#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[16]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[17]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[18]#011train-error:0#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[19]#011train-error:0#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[20]#011train-error:0#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[21]#011train-error:0#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[22]#011train-error:0#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[23]#011train-error:0#011validation-error:0.091371\u001b[0m\n",
      "\u001b[31m[24]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[25]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[26]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[27]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[28]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[29]#011train-error:0#011validation-error:0.086294\u001b[0m\n",
      "\u001b[31m[30]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[31]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[32]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[33]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[34]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[35]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[36]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[37]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[38]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[39]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[40]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[41]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[42]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[43]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[44]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[45]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[46]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[47]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[48]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[49]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[50]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[51]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[52]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[53]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[54]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[55]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[56]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[57]#011train-error:0#011validation-error:0.076142\u001b[0m\n",
      "\u001b[31m[58]#011train-error:0#011validation-error:0.076142\u001b[0m\n",
      "\u001b[31m[59]#011train-error:0#011validation-error:0.076142\u001b[0m\n",
      "\u001b[31m[60]#011train-error:0#011validation-error:0.076142\u001b[0m\n",
      "\u001b[31m[61]#011train-error:0#011validation-error:0.076142\u001b[0m\n",
      "\u001b[31m[62]#011train-error:0#011validation-error:0.076142\u001b[0m\n",
      "\u001b[31m[63]#011train-error:0#011validation-error:0.076142\u001b[0m\n",
      "\u001b[31m[64]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[65]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[66]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[67]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[68]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[69]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[70]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[71]#011train-error:0#011validation-error:0.076142\u001b[0m\n",
      "\u001b[31m[72]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[73]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[74]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[75]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[76]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[77]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[78]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[79]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[80]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[81]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[82]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[83]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[84]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[85]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[86]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[87]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[88]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[89]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[90]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[91]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[92]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[93]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[94]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[95]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[96]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[97]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[98]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\u001b[31m[99]#011train-error:0#011validation-error:0.081218\u001b[0m\n",
      "\n",
      "2019-10-20 09:33:09 Uploading - Uploading generated training model\n",
      "2019-10-20 09:33:09 Completed - Training job completed\n",
      "Training seconds: 62\n",
      "Billable seconds: 62\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(\n",
    "                        max_depth=10,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a note of the best validation error after the training is done\n",
    "\n",
    "This is to compare the default hyperparameters with the best job after hyperparameter tuning is finised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SageMaker Deployment\n",
    "\n",
    "Now that we've trained the algorithm, let's create a model and deploy it to a hosted endpoint.\n",
    "<img src=\"./images/deployment.png\" width=\"200\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.values[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values.  In this case, we're simply predicting whether the credit card transaction is Fraud (`1`) or not (`0`), which produces a simple confusion matrix.\n",
    "\n",
    "#### Print Confusion Matrix\n",
    "<img src=\"./images/Confusion_matrix.png\" width=\"200\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions  0.0  1.0\n",
       "actual               \n",
       "0             52    0\n",
       "1              3   44"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning:\n",
    "\n",
    "\n",
    "<img src=\"./images/model_tuning.png\" width=\"700\" height=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "### Tuning The Model - Hyperparameter Optimization (HPO)\n",
    "\n",
    "![HPO](./images/gif.gif \"HPO Experiment\")\n",
    "\n",
    "![HPO](./images/Optimized_Controller.gif \"HPO Experiment\")\n",
    "\n",
    "\n",
    "**Source: ** http://arxiv.org/abs/1509.01066 and https://www.youtube.com/watch?v=GiqNQdzc5TI\n",
    "\n",
    "\n",
    "Hyperparameter tuning is a supervised machine learning regression problem. Given a set of input features (the hyperparameters), hyperparameter tuning optimizes a model for the metric that you choose. hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get the best results, and runs training jobs to test these guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(1, 10)}\n",
    "objective_metric_name = 'validation:error'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=20,\n",
    "                            objective_type='Minimize',\n",
    "                            max_parallel_jobs=3)\n",
    "\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Hyperparameter Tuning Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=\"credit-fraud-detection-HPO-Job\")['HyperParameterTuningJobStatus']'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an inference request After Loading The Model\n",
    "\n",
    "Get the model artifact from S3 Location then the unpack it and load it to use it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model artifacts before running the next piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import xgboost as xgb\n",
    "transaction= \"-1.009630,0.141192,0.167167,-0.808785,2.112167,-1.294934,0.592454,-0.049872,-0.284882,-1.296757,-1.010293,-0.272631,-0.139809,-0.918097,-0.475136,0.519497,0.158822,-0.120745,-0.519128,0.108956,-0.225473,-0.947079,0.054725,0.368866,-0.158482,0.070904,0.022035,0.177674,-0.279746,0.391123\"\n",
    "test = transaction.split(',')\n",
    "data = np.asarray(test).reshape((1,-1))\n",
    "test_matrix = xgb.DMatrix(data)\n",
    "filename = \"./xgboost-model\"\n",
    "xgb_loaded = pickle.load(open(filename, 'rb'))\n",
    "predictions = xgb_loaded.predict(test_matrix)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interperting the Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_tree, Booster\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from xgboost import plot_tree, plot_importance\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "filename='./xgboost-model'\n",
    "# plot single tree\n",
    "rcParams['figure.figsize'] = 50,50\n",
    " \n",
    "model = pkl.load(open(filename,'rb')) \n",
    "plot_tree(model, num_trees=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, Let's Deploy The Model in Lambda and API Gateway\n",
    "\n",
    "Using Cloud9 which is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you donâ€™t need to install files or configure your development machine to start new projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources:\n",
    "\n",
    "- Amazon Sagemaker: https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html\n",
    "- XGBoost Algorithm: https://xgboost.readthedocs.io/en/latest/\n",
    "- Oversampling vs Undersampling: https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis\n",
    "- Why Correlation Matters: https://towardsdatascience.com/why-feature-correlation-matters-a-lot-847e8ba439c4\n",
    "- Correlation Matrix: https://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_matrices\n",
    "- Hyperparameters Optimization: https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'prefix' (str)\n",
      "Stored 'bucket' (str)\n",
      "Stored 's3_input_train' (s3_input)\n",
      "Stored 's3_input_validation' (s3_input)\n"
     ]
    }
   ],
   "source": [
    "%store prefix\n",
    "%store bucket\n",
    "%store s3_input_train\n",
    "%store s3_input_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
